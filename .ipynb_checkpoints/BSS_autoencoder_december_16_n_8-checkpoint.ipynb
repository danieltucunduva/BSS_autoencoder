{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blind source separation with an unsupervised autoencoder\n",
    "\n",
    "# Daniel Correa Tucunduva \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_layer_n = 8\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mne.io import read_raw_eeglab\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load real EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading .\\outside_scanner.fdt\n",
      "Reading 0 ... 124404  =      0.000 ...   497.616 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-5a9b7a979c0e>:1: RuntimeWarning: The data contains 'boundary' events, indicating data discontinuities. Be cautious of filtering and epoching around these events.\n",
      "  raw_eeg_lab = read_raw_eeglab('./outside_scanner.set')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting \"time\" to \"<class 'numpy.int64'>\"...\n",
      "(124405, 64)\n"
     ]
    }
   ],
   "source": [
    "raw_eeg_lab = read_raw_eeglab('./outside_scanner.set')\n",
    "raw_eeg_lab.load_data()\n",
    "np_eeg_data = raw_eeg_lab.to_data_frame()\n",
    "print(np_eeg_data.shape)\n",
    "\n",
    "n = np_eeg_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ica = FastICA(n_components=3)  # we'll be generous and assume there's 3\n",
    "# ica.fit(reshape_mixed.T)  # .T because 100000 instances, 100 attributes\n",
    "# ica_comps = ica.transform(reshape_mixed.T)  # get the components\n",
    "# ica_weights = np.reshape(ica.components_.T, [10, 10, 3])  # visualize these as images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8)                 512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                512       \n",
      "=================================================================\n",
      "Total params: 1,024\n",
      "Trainable params: 1,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Sequential()\n",
    "autoencoder.name = 'autoencoder'\n",
    "autoencoder.add(Dense(hidden_layer_n, activation='linear', input_shape=[64], kernel_initializer='uniform', use_bias=False, kernel_constraint='nonneg'))\n",
    "autoencoder.add(Dense(64, activation='linear', kernel_initializer='uniform', use_bias=False, kernel_constraint='nonneg'))\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-1, momentum=0.0, nesterov=False)\n",
    "\n",
    "autoencoder.compile(loss='logcosh', optimizer=optimizer)\n",
    "\n",
    "autoencoder_input = np_eeg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 15.4112\n",
      "Epoch 2/100\n",
      "124405/124405 [==============================] - 2s 20us/step - loss: 13.2751\n",
      "Epoch 3/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 13.1393  - ETA\n",
      "Epoch 4/100\n",
      "124405/124405 [==============================] - 2s 20us/step - loss: 12.8555\n",
      "Epoch 5/100\n",
      "124405/124405 [==============================] - 2s 20us/step - loss: 12.4051\n",
      "Epoch 6/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 12.0346\n",
      "Epoch 7/100\n",
      "124405/124405 [==============================] - 2s 20us/step - loss: 11.8067\n",
      "Epoch 8/100\n",
      "124405/124405 [==============================] - 2s 20us/step - loss: 11.6403\n",
      "Epoch 9/100\n",
      "124405/124405 [==============================] - 2s 20us/step - loss: 11.4965\n",
      "Epoch 10/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 11.3673\n",
      "Epoch 11/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 11.2597\n",
      "Epoch 12/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 11.1659\n",
      "Epoch 13/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 11.0785\n",
      "Epoch 14/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 10.9951\n",
      "Epoch 15/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 10.9153\n",
      "Epoch 16/100\n",
      "124405/124405 [==============================] - 3s 22us/step - loss: 10.8374\n",
      "Epoch 17/100\n",
      "124405/124405 [==============================] - 3s 27us/step - loss: 10.7564\n",
      "Epoch 18/100\n",
      "124405/124405 [==============================] - 3s 25us/step - loss: 10.6711\n",
      "Epoch 19/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.5868\n",
      "Epoch 20/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.4976\n",
      "Epoch 21/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.4440\n",
      "Epoch 22/100\n",
      "124405/124405 [==============================] - 3s 22us/step - loss: 10.4341\n",
      "Epoch 23/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.4242\n",
      "Epoch 24/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.4147\n",
      "Epoch 25/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.4053\n",
      "Epoch 26/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.3958\n",
      "Epoch 27/100\n",
      "124405/124405 [==============================] - 3s 25us/step - loss: 10.3863\n",
      "Epoch 28/100\n",
      "124405/124405 [==============================] - 3s 27us/step - loss: 10.3768\n",
      "Epoch 29/100\n",
      "124405/124405 [==============================] - 3s 23us/step - loss: 10.3672\n",
      "Epoch 30/100\n",
      "124405/124405 [==============================] - 3s 27us/step - loss: 10.3580\n",
      "Epoch 31/100\n",
      "124405/124405 [==============================] - 3s 25us/step - loss: 10.3490\n",
      "Epoch 32/100\n",
      "124405/124405 [==============================] - 3s 22us/step - loss: 10.3399\n",
      "Epoch 33/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.3308\n",
      "Epoch 34/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.3217\n",
      "Epoch 35/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.3125\n",
      "Epoch 36/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.3033\n",
      "Epoch 37/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 10.2940\n",
      "Epoch 38/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 10.2846\n",
      "Epoch 39/100\n",
      "124405/124405 [==============================] - 3s 21us/step - loss: 10.2751\n",
      "Epoch 40/100\n",
      "124405/124405 [==============================] - 3s 20us/step - loss: 10.2656\n",
      "Epoch 41/100\n",
      "123200/124405 [============================>.] - ETA: 0s - loss: 10.2612"
     ]
    }
   ],
   "source": [
    "        \n",
    "def learning_rate_schedule(epoch):\n",
    "    e = epoch + 1\n",
    "    if e > 80:\n",
    "        return 1e-8\n",
    "    if e > 60:\n",
    "        return 1e-7\n",
    "    if e > 40:\n",
    "        return 1e-6\n",
    "    if e > 20:\n",
    "        return 1e-5\n",
    "    return 1e-4\n",
    "\n",
    "    \n",
    "autoencoder_history = autoencoder.fit(autoencoder_input,\n",
    "                                      autoencoder_input,\n",
    "                                      batch_size=40,\n",
    "                                      epochs=epochs,\n",
    "                                      verbose=1,\n",
    "                                      callbacks=[\n",
    "                                      LearningRateScheduler(learning_rate_schedule)\n",
    "                                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reconstruction = autoencoder.predict(autoencoder_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original and reconstruction comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Recover independent sources from hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hidden_layer = Sequential()\n",
    "hidden_layer.name = 'hidden_layer'\n",
    "hidden_layer.add(Dense(hidden_layer_n, input_shape=[64], use_bias=False, kernel_constraint='nonneg'))\n",
    "hidden_layer.summary()\n",
    "\n",
    "hidden_layer.set_weights(autoencoder.get_weights())\n",
    "\n",
    "recovered_sources = hidden_layer.predict(autoencoder_input)\n",
    "\n",
    "names = []\n",
    "sources = []\n",
    "for i in range(hidden_layer_n):\n",
    "    names.append('source_' + str(i))\n",
    "    sources.append(np.reshape(recovered_sources.T[i], [n, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICA and autoencoder results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "models = sources\n",
    "colors = ['red', 'steelblue', 'orange']\n",
    "\n",
    "for index, (model, name) in enumerate(zip(models, names), 1):\n",
    "    plt.subplot(hidden_layer_n * 2 + 2, 1, index * 2)\n",
    "    plt.title(name)\n",
    "    for sig, color in zip(model.T, colors):\n",
    "        plt.plot(sig[0:2000], color=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
