{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blind source separation with an autoencoder\n",
    "\n",
    "#### Author\n",
    "<pre style=\"font-size: 120%\">\n",
    "Daniel Correa Tucunduva \n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Module setup\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator\n",
    "from keras.layers import LSTM, Activation, Flatten, Dropout, BatchNormalization, Input, Dense, concatenate, Conv2D, Conv3D, MaxPooling2D, UpSampling2D\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "smoothing...\n",
      "smoothing...\n",
      "smoothing...\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "\"\\nput code down here to unmix the sources using a neural network. \\ndo not use the original sources in the training of the network. \\nyou can use the original sources to verify the quality of the network's reconstruction\\nyou can start by creating a simple autoencoder (non-CNN) to see first if you can just\\nreconstruct the orginal input using a hidden-layer representation with 3 units, use\\nthe reshaped vector as (line 51) as input to the network as for ICA.\\nif you are able to reconstruct the input well enough using the autoencoder, find a way\\nto obtain a hidden-layer representation of the time series, to see if the network is\\nactually capturing independent sources in the hidden layer units (it probably won't)\\nthen move to more complex architectures (CNN). i'm hoping CNN will learn independent sources \\nin the compressed hidden layer, because the data is spatially correlated\\n\""
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "grid = np.zeros([10, 10])\n",
    "\n",
    "src_loc_1 = [3, 3]\n",
    "src_loc_2 = [5, 7]\n",
    "src_loc_3 = [8, 2]\n",
    "\n",
    "src_1 = np.sin(np.arange(0, 1000, 0.01))  # sin\n",
    "\n",
    "src_2 = np.zeros(src_1.shape[0])  # box\n",
    "for i in np.arange(0, src_2.shape[0], 200):\n",
    "    src_2[i:i + 100] = 1\n",
    "\n",
    "src_3 = np.zeros(src_1.shape[0])  # sawtooth\n",
    "for i in np.arange(0, src_3.shape[0], 200):\n",
    "    src_3[i:i + 200] = np.arange(0, 200) / 200\n",
    "\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "def smooth_src(input_signal, sigma):\n",
    "    print('smoothing...')\n",
    "    for i in np.arange(0, input_signal.shape[2]):\n",
    "        input_signal[:, :, i] = gaussian_filter(input_signal[:, :, i], sigma)\n",
    "\n",
    "    return input_signal\n",
    "\n",
    "\n",
    "src_grid_1 = np.zeros([10, 10, src_1.shape[0]])  # init 3d grid\n",
    "src_grid_1[src_loc_1[0], src_loc_1[1], :] = src_1  # place point source in grid\n",
    "src_grid_1 = smooth_src(src_grid_1, 3)  # smooth\n",
    "\n",
    "src_grid_2 = np.zeros([10, 10, src_2.shape[0]])\n",
    "src_grid_2[src_loc_2[0], src_loc_2[1], :] = src_2\n",
    "src_grid_2 = smooth_src(src_grid_2, 3)\n",
    "\n",
    "src_grid_3 = np.zeros([10, 10, src_3.shape[0]])\n",
    "src_grid_3[src_loc_3[0], src_loc_3[1], :] = src_3\n",
    "src_grid_3 = smooth_src(src_grid_3, 3)\n",
    "\n",
    "mixed = src_grid_1 + src_grid_2 + src_grid_3  # goal is to unmix mixed\n",
    "\"\"\"\n",
    "the matrix 'mixed' is the sum of all 3 sources, where each source is a 2d gaussian\n",
    "smoothed time series (sin, boxcar, sawtooth). the goal is to recover the original\n",
    "time series, as well as the spatial topography of the original sources  \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "reshape_mixed = np.reshape(mixed, [100, mixed.shape[2]])\n",
    "ica = FastICA(n_components=3)  # we'll be generous and assume there's 3\n",
    "ica.fit(reshape_mixed.T)  # .T because 100000 instances, 100 attributes\n",
    "ica_comps = ica.transform(reshape_mixed.T)  # get the components\n",
    "ica_weights = np.reshape(ica.components_.T, [10, 10, 3])  # visualize these as images\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "put code down here to unmix the sources using a neural network. \n",
    "do not use the original sources in the training of the network. \n",
    "you can use the original sources to verify the quality of the network's reconstruction\n",
    "you can start by creating a simple autoencoder (non-CNN) to see first if you can just\n",
    "reconstruct the orginal input using a hidden-layer representation with 3 units, use\n",
    "the reshaped vector as (line 51) as input to the network as for ICA.\n",
    "if you are able to reconstruct the input well enough using the autoencoder, find a way\n",
    "to obtain a hidden-layer representation of the time series, to see if the network is\n",
    "actually capturing independent sources in the hidden layer units (it probably won't)\n",
    "then move to more complex architectures (CNN). i'm hoping CNN will learn independent sources \n",
    "in the compressed hidden layer, because the data is spatially correlated\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 10, 10, 1)    0                                            \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 10, 10, 20)   10020       input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 10, 10, 20)   14020       input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 10, 10, 20)   20020       input_1[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 10, 10, 20)   0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 20)   0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 10, 10, 20)   0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 30, 10, 20)   0           max_pooling2d_1[0][0]            \n                                                                 max_pooling2d_2[0][0]            \n                                                                 max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nflatten_5 (Flatten)             (None, 6000)         0           concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 200)          1200200     flatten_5[0][0]                  \n__________________________________________________________________________________________________\ndense_6 (Dense)                 (None, 10)           2010        dense_5[0][0]                    \n==================================================================================================\nTotal params: 1,246,270\nTrainable params: 1,246,270\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# input_shape = Input(shape=(10, 10, 1))\n",
    "# \n",
    "# tower_1 = Conv2D(20, (100, 5), padding='same', activation='relu')(input_shape)\n",
    "# tower_1 = MaxPooling2D((1, 11), strides=(1, 1), padding='same')(tower_1)\n",
    "# \n",
    "# tower_2 = Conv2D(20, (100, 7), padding='same', activation='relu')(input_shape)\n",
    "# tower_2 = MaxPooling2D((1, 9), strides=(1, 1), padding='same')(tower_2)\n",
    "# \n",
    "# tower_3 = Conv2D(20, (100, 10), padding='same', activation='relu')(input_shape)\n",
    "# tower_3 = MaxPooling2D((1, 6), strides=(1, 1), padding='same')(tower_3)\n",
    "# \n",
    "# merged = keras.layers.concatenate([tower_1, tower_2, tower_3], axis=1)\n",
    "# merged = Flatten()(merged)\n",
    "# \n",
    "# out = Dense(200, activation='relu')(merged)\n",
    "# out = Dense(10, activation='softmax')(out)\n",
    "# \n",
    "# model = Model(input_shape, out)\n",
    "# \n",
    "# model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_7 (Dense)              (None, 3)                 303       \n_________________________________________________________________\ndense_8 (Dense)              (None, 100)               400       \n=================================================================\nTotal params: 703\nTrainable params: 703\nNon-trainable params: 0\n_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# dense = Sequential()\n",
    "# dense.add(Dense(3, input_shape=[100]))\n",
    "# dense.add(Dense(100, activation='linear'))\n",
    "# \n",
    "# dense.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# weight_decay = 1e-4\n",
    "# cnn = Sequential()\n",
    "# cnn.add(Conv2D(16, (3, 3), strides=2, padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=[10, 10, 1]))\n",
    "# cnn.add(Activation('elu'))\n",
    "# cnn.add(BatchNormalization())\n",
    "# cnn.add(Conv2D(16, (3, 3), strides=2, padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "# cnn.add(Activation('elu'))\n",
    "# cnn.add(BatchNormalization())\n",
    "# cnn.add(Conv2D(8, (3, 3), strides=2, padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "# cnn.add(Activation('elu'))\n",
    "# cnn.add(Conv2D(8, (2, 2), strides=2, padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "# cnn.add(Activation('elu'))\n",
    "# \n",
    "# cnn.add(Flatten())\n",
    "# cnn.add(Dense(100, activation='linear'))\n",
    "# \n",
    "# cnn.summary()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv3d_13 (Conv3D)           (None, 4, 4, 100, 3)      195       \n_________________________________________________________________\nactivation_15 (Activation)   (None, 4, 4, 100, 3)      0         \n_________________________________________________________________\nconv3d_14 (Conv3D)           (None, 2, 2, 100, 3)      579       \n_________________________________________________________________\nactivation_16 (Activation)   (None, 2, 2, 100, 3)      0         \n_________________________________________________________________\nbatch_normalization_4 (Batch (None, 2, 2, 100, 3)      12        \n_________________________________________________________________\nconv3d_15 (Conv3D)           (None, 1, 1, 100, 3)      579       \n_________________________________________________________________\nactivation_17 (Activation)   (None, 1, 1, 100, 3)      0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 300)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 100)               30100     \n=================================================================\nTotal params: 31,465\nTrainable params: 31,459\nNon-trainable params: 6\n_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# weight_decay = 1e-4\n",
    "# cnn = Sequential()\n",
    "# cnn.add(Conv3D(3, (4, 4, 4), strides=(2, 2, 1), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=[8, 8, 100, 1]))\n",
    "# cnn.add(Activation('elu'))\n",
    "# cnn.add(Conv3D(3, (4, 4, 4), strides=(2, 2, 1), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "# cnn.add(Activation('elu'))\n",
    "# cnn.add(BatchNormalization())\n",
    "# cnn.add(Conv3D(3, (4, 4, 4), strides=(2, 2, 1), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "# cnn.add(Activation('elu'))\n",
    "# cnn.add(Flatten())\n",
    "# cnn.add(Dense(100, activation='linear'))\n",
    "# \n",
    "# cnn.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "\r  100/10000 [..............................] - ETA: 52s - loss: 3.7137e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  500/10000 [>.............................] - ETA: 11s - loss: 3.4844e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  900/10000 [=>............................] - ETA: 6s - loss: 3.3498e-04 ",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 1300/10000 [==>...........................] - ETA: 4s - loss: 3.2359e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 1700/10000 [====>.........................] - ETA: 3s - loss: 3.1631e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2000/10000 [=====>........................] - ETA: 3s - loss: 3.0984e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2300/10000 [=====>........................] - ETA: 2s - loss: 3.0474e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2700/10000 [=======>......................] - ETA: 2s - loss: 2.9784e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3100/10000 [========>.....................] - ETA: 2s - loss: 2.9111e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 3500/10000 [=========>....................] - ETA: 2s - loss: 2.8522e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3900/10000 [==========>...................] - ETA: 1s - loss: 2.7939e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4300/10000 [===========>..................] - ETA: 1s - loss: 2.7331e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4700/10000 [=============>................] - ETA: 1s - loss: 2.6757e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5100/10000 [==============>...............] - ETA: 1s - loss: 2.6281e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5500/10000 [===============>..............] - ETA: 1s - loss: 2.5774e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5900/10000 [================>.............] - ETA: 0s - loss: 2.5292e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6300/10000 [=================>............] - ETA: 0s - loss: 2.4836e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6700/10000 [===================>..........] - ETA: 0s - loss: 2.4405e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7100/10000 [====================>.........] - ETA: 0s - loss: 2.3990e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7500/10000 [=====================>........] - ETA: 0s - loss: 2.3599e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7900/10000 [======================>.......] - ETA: 0s - loss: 2.3201e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8300/10000 [=======================>......] - ETA: 0s - loss: 2.2816e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8700/10000 [=========================>....] - ETA: 0s - loss: 2.2480e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9100/10000 [==========================>...] - ETA: 0s - loss: 2.2128e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9500/10000 [===========================>..] - ETA: 0s - loss: 2.1796e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9900/10000 [============================>.] - ETA: 0s - loss: 2.1471e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r10000/10000 [==============================] - 2s 195us/step - loss: 2.1401e-04\n",
      "Epoch 2/5\n\r  100/10000 [..............................] - ETA: 1s - loss: 1.5137e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r  500/10000 [>.............................] - ETA: 1s - loss: 1.3877e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r  900/10000 [=>............................] - ETA: 1s - loss: 1.3782e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 1300/10000 [==>...........................] - ETA: 1s - loss: 1.3670e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1700/10000 [====>.........................] - ETA: 1s - loss: 1.3499e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2100/10000 [=====>........................] - ETA: 1s - loss: 1.3374e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2500/10000 [======>.......................] - ETA: 0s - loss: 1.3225e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2900/10000 [=======>......................] - ETA: 0s - loss: 1.3066e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3300/10000 [========>.....................] - ETA: 0s - loss: 1.2923e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3700/10000 [==========>...................] - ETA: 0s - loss: 1.2805e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4100/10000 [===========>..................] - ETA: 0s - loss: 1.2737e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4500/10000 [============>.................] - ETA: 0s - loss: 1.2626e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4900/10000 [=============>................] - ETA: 0s - loss: 1.2525e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5300/10000 [==============>...............] - ETA: 0s - loss: 1.2460e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5700/10000 [================>.............] - ETA: 0s - loss: 1.2392e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6100/10000 [=================>............] - ETA: 0s - loss: 1.2314e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6500/10000 [==================>...........] - ETA: 0s - loss: 1.2281e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6900/10000 [===================>..........] - ETA: 0s - loss: 1.2221e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7300/10000 [====================>.........] - ETA: 0s - loss: 1.2173e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7700/10000 [======================>.......] - ETA: 0s - loss: 1.2140e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8100/10000 [=======================>......] - ETA: 0s - loss: 1.2106e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8500/10000 [========================>.....] - ETA: 0s - loss: 1.2067e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8900/10000 [=========================>....] - ETA: 0s - loss: 1.2060e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9300/10000 [==========================>...] - ETA: 0s - loss: 1.2048e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9700/10000 [============================>.] - ETA: 0s - loss: 1.2002e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r10000/10000 [==============================] - 1s 133us/step - loss: 1.1992e-04\n",
      "Epoch 3/5\n\r  100/10000 [..............................] - ETA: 1s - loss: 1.1044e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  500/10000 [>.............................] - ETA: 1s - loss: 1.1365e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  900/10000 [=>............................] - ETA: 1s - loss: 1.1242e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1300/10000 [==>...........................] - ETA: 1s - loss: 1.1308e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1700/10000 [====>.........................] - ETA: 1s - loss: 1.1276e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2100/10000 [=====>........................] - ETA: 1s - loss: 1.1364e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2500/10000 [======>.......................] - ETA: 0s - loss: 1.1323e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2900/10000 [=======>......................] - ETA: 0s - loss: 1.1212e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3300/10000 [========>.....................] - ETA: 0s - loss: 1.1237e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3700/10000 [==========>...................] - ETA: 0s - loss: 1.1302e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4100/10000 [===========>..................] - ETA: 0s - loss: 1.1275e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4500/10000 [============>.................] - ETA: 0s - loss: 1.1289e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4900/10000 [=============>................] - ETA: 0s - loss: 1.1278e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5300/10000 [==============>...............] - ETA: 0s - loss: 1.1272e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5700/10000 [================>.............] - ETA: 0s - loss: 1.1261e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6100/10000 [=================>............] - ETA: 0s - loss: 1.1291e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6500/10000 [==================>...........] - ETA: 0s - loss: 1.1308e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6900/10000 [===================>..........] - ETA: 0s - loss: 1.1286e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7300/10000 [====================>.........] - ETA: 0s - loss: 1.1299e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7700/10000 [======================>.......] - ETA: 0s - loss: 1.1292e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8100/10000 [=======================>......] - ETA: 0s - loss: 1.1317e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8500/10000 [========================>.....] - ETA: 0s - loss: 1.1320e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8900/10000 [=========================>....] - ETA: 0s - loss: 1.1327e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9300/10000 [==========================>...] - ETA: 0s - loss: 1.1325e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9700/10000 [============================>.] - ETA: 0s - loss: 1.1307e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10000/10000 [==============================] - 1s 133us/step - loss: 1.1291e-04\n",
      "Epoch 4/5\n\r  100/10000 [..............................] - ETA: 1s - loss: 1.1285e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  500/10000 [>.............................] - ETA: 1s - loss: 1.1134e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  900/10000 [=>............................] - ETA: 1s - loss: 1.1511e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1300/10000 [==>...........................]",
      " - ETA: 1s - loss: 1.1383e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1700/10000 [====>.........................] - ETA: 1s - loss: 1.1321e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2100/10000 [=====>........................] - ETA: 1s - loss: 1.1307e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2500/10000 [======>.......................] - ETA: 1s - loss: 1.1244e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2900/10000 [=======>......................] - ETA: 0s - loss: 1.1196e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3300/10000 [========>.....................] - ETA: 0s - loss: 1.1216e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3700/10000 [==========>...................] - ETA: 0s - loss: 1.1227e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4100/10000 [===========>..................] - ETA: 0s - loss: 1.1229e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4500/10000 [============>.................] - ETA: 0s - loss: 1.1261e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4900/10000 [=============>................] - ETA: 0s - loss: 1.1231e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5300/10000 [==============>...............] - ETA: 0s - loss: 1.1219e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5700/10000 [================>.............] - ETA: 0s - loss: 1.1233e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6100/10000 [=================>............] - ETA: 0s - loss: 1.1239e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6500/10000 [==================>...........] - ETA: 0s - loss: 1.1264e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6900/10000 [===================>..........] - ETA: 0s - loss: 1.1280e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7300/10000 [====================>.........] - ETA: 0s - loss: 1.1260e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7700/10000 [======================>.......] - ETA: 0s - loss: 1.1253e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8100/10000 [=======================>......] - ETA: 0s - loss: 1.1274e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8500/10000 [========================>.....] - ETA: 0s - loss: 1.1264e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8900/10000 [=========================>....] - ETA: 0s - loss: 1.1286e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9300/10000 [==========================>...] - ETA: 0s - loss: 1.1295e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 9700/10000 [============================>.] - ETA: 0s - loss: 1.1317e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r10000/10000 [==============================] - 1s 134us/step - loss: 1.1310e-04\n",
      "Epoch 5/5\n\r  100/10000 [..............................] - ETA: 1s - loss: 1.0205e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  500/10000 [>.............................] - ETA: 1s - loss: 1.1337e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  900/10000 [=>............................] - ETA: 1s - loss: 1.1360e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1300/10000 [==>...........................] - ETA: 1s - loss: 1.1261e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 1700/10000 [====>.........................] - ETA: 1s - loss: 1.1318e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2100/10000 [=====>........................] - ETA: 1s - loss: 1.1222e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2500/10000 [======>.......................] - ETA: 1s - loss: 1.1250e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 2900/10000 [=======>......................] - ETA: 0s - loss: 1.1258e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3300/10000 [========>.....................] - ETA: 0s - loss: 1.1223e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3700/10000 [==========>...................] - ETA: 0s - loss: 1.1268e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4100/10000 [===========>..................] - ETA: 0s - loss: 1.1255e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4500/10000 [============>.................] - ETA: 0s - loss: 1.1283e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 4900/10000 [=============>................] - ETA: 0s - loss: 1.1264e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5300/10000 [==============>...............] - ETA: 0s - loss: 1.1260e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 5700/10000 [================>.............] - ETA: 0s - loss: 1.1248e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 6100/10000 [=================>............] - ETA: 0s - loss: 1.1259e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 6500/10000 [==================>...........] - ETA: 0s - loss: 1.1258e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 6900/10000 [===================>..........] - ETA: 0s - loss: 1.1257e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 7300/10000 [====================>.........] - ETA: 0s - loss: 1.1231e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 7700/10000 [======================>.......] - ETA: 0s - loss: 1.1223e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 8100/10000 [=======================>......] - ETA: 0s - loss: 1.1209e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 8500/10000 [========================>.....] - ETA: 0s - loss: 1.1225e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 8900/10000 [=========================>....] - ETA: 0s - loss: 1.1232e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 9300/10000 [==========================>...] - ETA: 0s - loss: 1.1257e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b",
      "\r 9700/10000 [============================>.] - ETA: 0s - loss: 1.1278e-04",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r10000/10000 [==============================] - 1s 143us/step - loss: 1.1287e-04\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x22123366dd8>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 73
    }
   ],
   "source": [
    "# def learning_rate_schedule(epoch):\n",
    "#     if epoch > 30:\n",
    "#         return 0.00025\n",
    "#     if epoch > 20:\n",
    "#         return 0.0005\n",
    "#     return 0.001\n",
    "# \n",
    "# optimizer = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "# cnn.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "# \n",
    "# sliced = np.reshape(mixed, [10000, 10, 10, 10, 1])\n",
    "# \n",
    "# target = np.reshape(reshape_mixed.T, [10000, 1000])\n",
    "#     \n",
    "# \n",
    "# cnn.fit(sliced,\n",
    "#         target,\n",
    "#         epochs=5,\n",
    "#         batch_size=100,\n",
    "#         shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "# cnn.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "# \n",
    "# def learning_rate_schedule(epoch):\n",
    "#     if epoch > 30:\n",
    "#         return 0.00025\n",
    "#     if epoch > 20:\n",
    "#         return 0.0005\n",
    "#     return 0.001\n",
    "# \n",
    "# cnn_mixed = np.reshape(mixed, [1, 10, 10, 100000])\n",
    "# \n",
    "# print(cnn_mixed.shape)\n",
    "# \n",
    "# generator = ImageDataGenerator()\n",
    "# history_cnn = cnn.fit_generator(\n",
    "#     generator.flow(cnn_mixed.T, reshape_mixed.T, batch_size=1000),\n",
    "#     steps_per_epoch=100,\n",
    "#     epochs=10,\n",
    "#     verbose=1,\n",
    "#     callbacks=[LearningRateScheduler(learning_rate_schedule)],\n",
    "#     shuffle=True\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"sequential_8\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_13 (Dense)             (None, 3)                 303       \n_________________________________________________________________\ndense_14 (Dense)             (None, 100)               400       \n=================================================================\nTotal params: 703\nTrainable params: 703\nNon-trainable params: 0\n_________________________________________________________________\nModel: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_15 (Dense)             (None, 3)                 303       \n=================================================================\nTotal params: 303\nTrainable params: 303\nNon-trainable params: 0\n_________________________________________________________________\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "dense = Sequential()\n",
    "dense.add(Dense(3, input_shape=[100]))\n",
    "dense.add(Dense(100, activation='linear'))\n",
    "\n",
    "dense.summary()\n",
    "\n",
    "hidden = Sequential()\n",
    "hidden.add(Dense(3, input_shape=[100]))\n",
    "\n",
    "hidden.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 1/5\n\r1/1 [==============================] - 0s 191ms/step - loss: 4.6131\n",
      "Epoch 2/5\n\r1/1 [==============================] - 0s 1ms/step - loss: 4.5739\n",
      "Epoch 3/5\n\r1/1 [==============================] - 0s 982us/step - loss: 4.5479\n",
      "Epoch 4/5\n\r1/1 [==============================] - 0s 998us/step - loss: 4.5272\n",
      "Epoch 5/5\n\r1/1 [==============================] - 0s 2ms/step - loss: 4.5096\n",
      "0.0 1.0 0.0\n[[-0.07401344 -0.0453392  -0.1696673 ]]\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n",
    "dense.compile(loss='kullback_leibler_divergence', optimizer=optimizer)\n",
    "\n",
    "frame = np.reshape(reshape_mixed.T[0], [1, 100])\n",
    "\n",
    "def learning_rate_schedule(epoch):\n",
    "    if epoch > 50:\n",
    "        return 0.00025\n",
    "    if epoch > 10:\n",
    "        return 0.0005\n",
    "    return 0.001\n",
    "\n",
    "dense_history = dense.fit(frame,\n",
    "                          frame,    \n",
    "                          epochs=5,    \n",
    "                          verbose=1)\n",
    "\n",
    "hidden.set_weights(dense.get_weights())\n",
    "\n",
    "print(src_1[0], src_2[0], src_3[0],)\n",
    "print(hidden.predict(frame))\n",
    "\n",
    "dense_json = dense.to_json()\n",
    "with open('dense.json', 'w') as json_file:\n",
    "    json_file.write(dense_json)\n",
    "dense.save_weights('dense.h5')\n",
    "\n",
    "prediction = dense.predict(frame)\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "plt.imshow(np.reshape(frame, [10, 10]))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.imshow(np.reshape(prediction, [10, 10]))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# prediction = dense.predict(frame)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "# prediction = cnn.predict(sliced)\n",
    "# \n",
    "# pred = np.reshape(prediction[0], [10, 10, 10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(10000, 1000)\n(10, 10, 10)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# print(prediction.shape)\n",
    "# print(pred.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1440x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "j = 1\n",
    "# for i in [0, 10, 100, 500, 3000, 9999]:\n",
    "    # display original\n",
    "ax = plt.subplot(2, 10, j)\n",
    "plt.imshow(np.reshape(frame, [10, 10]))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "\n",
    "# display reconstruction\n",
    "ax = plt.subplot(2, 10, j + 10)\n",
    "plt.imshow(np.reshape(prediction, [10, 10]))\n",
    "plt.gray()\n",
    "ax.get_xaxis().set_visible(False)\n",
    "ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # j += 1\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# hidden_layer_output_model = Sequential()\n",
    "# hidden_layer_output_model.add(Dense(1, input_shape=[100]))\n",
    "# \n",
    "# hidden_layer_output_model.summary()\n",
    "# \n",
    "# hidden_layer_output_model.set_weights(dense.layers[0].get_weights())\n",
    "# \n",
    "# hidden_layer_output_model.compile(loss='mean_squared_error', optimizer=optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# hidden_layer_output = hidden_layer_output_model.predict(reshape_mixed.T)\n",
    "# \n",
    "# print(hidden_layer_output[100])\n",
    "# print(src_grid_1[3, 3, 100], src_grid_2[5, 7, 0], src_grid_3[8, 2, 100])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}